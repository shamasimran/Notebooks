{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 21_delta_advanced\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Advanced operations with Delta Lake\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Implemented upserts (merge) in Delta  \n",
    "                                              Used time travel queries  \n",
    "                                              Performed vacuum and optimize  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b49852-ecc8-4799-886d-62b7f8d904ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Spark Session\n",
    "# ------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Delta_Advanced_Features\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34026548-2a9e-4337-8b94-ce3c09ee8884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2) Sample Student DataFrame\n",
    "# ------------------------------------------------------------\n",
    "student_schema = StructType([\n",
    "    StructField('StudentID', IntegerType(), False),\n",
    "    StructField('StudentName', StringType(), True),\n",
    "    StructField('StudentAge', IntegerType(), True)\n",
    "])\n",
    "\n",
    "student_data = [\n",
    "    (1, \"Alice\", 34),\n",
    "    (2, \"Bob\", 45),\n",
    "    (3, \"Charlie\", 29),\n",
    "    (4, \"Shamas\", 40)\n",
    "]\n",
    "\n",
    "df_student = spark.createDataFrame(student_data, student_schema)\n",
    "\n",
    "# Delta table path\n",
    "delta_path = \"/Volumes/datapurcatalog/default/datapurvolume/delta/student_table_advanced\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Create Delta Table\n",
    "# ------------------------------------------------------------\n",
    "df_student.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "# Now df_student is saved as Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5b563f-3b8e-4a52-b4f0-fc6466e6f3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                            # ------------------------------------------------------------\n",
    "                                                            # 4) Time Travel Queries\n",
    "                                                            # ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601910ae-9d10-4785-898b-381c1c140c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show latest version\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e47aedb-a76c-4c54-9d52-6396d3911824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get full history\n",
    "full_history = delta_table.history(1000)  # large number to cover all versions\n",
    "full_history.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc187c4e-d00c-43c6-a224-43cb3393299f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example: Read data as it was at version 0\n",
    "df_version0 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path)\n",
    "df_version0.show()\n",
    "# Key Point: Time travel allows you to query historical snapshots of Delta table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bddc7f3-5cb4-490b-9d97-a4e62ab2952b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5) Upserts / MERGE INTO (handling CDC)\n",
    "# ------------------------------------------------------------\n",
    "# Example new data to upsert (some new, some existing StudentID)\n",
    "student_updates = [\n",
    "    (3, \"Charlie\", 30),  # existing StudentID, age updated\n",
    "    (5, \"Faizan\", 25)    # new StudentID\n",
    "]\n",
    "\n",
    "df_updates = spark.createDataFrame(student_updates, student_schema)\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    df_updates.alias(\"src\"),\n",
    "    \"tgt.StudentID = src.StudentID\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"StudentName\": \"src.StudentName\",\n",
    "    \"StudentAge\": \"src.StudentAge\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"StudentID\": \"src.StudentID\",\n",
    "    \"StudentName\": \"src.StudentName\",\n",
    "    \"StudentAge\": \"src.StudentAge\"\n",
    "}).execute()\n",
    "\n",
    "# Show updated table\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584502b9-b90b-4b3d-8555-1e93224c998a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all files in Delta folder\n",
    "all_files = dbutils.fs.ls(delta_path)\n",
    "\n",
    "# Filter only Parquet files\n",
    "parquet_files = [f.path for f in all_files if f.path.endswith(\".parquet\")]\n",
    "\n",
    "# Show all Parquet files\n",
    "print(\"Parquet files in Delta folder:\")\n",
    "for file in parquet_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca224ec-8c0d-4301-8635-15d534515863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                    # ------------------------------------------------------------\n",
    "                                                    # 6.1) OPTIMIZE\n",
    "                                                    # ------------------------------------------------------------\n",
    "# OPTIMIZE (Delta Lake only in Databricks) improves query performance by compacting small files\n",
    "# delta_table.optimize().execute()   # Uncomment if using Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a089e9-107a-4c73-9fec-614fea4772c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                    # ------------------------------------------------------------\n",
    "                                                    # 6.2) VACUUM\n",
    "                                                    # ------------------------------------------------------------\n",
    "# VACUUM removes old files to clean up storage\n",
    "delta_table.vacuum(retentionHours=1)  # retentionHours can be set to retain files for safety\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce291789-cc5d-41d8-9f47-140b0a16403f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7) Schema Evolution\n",
    "# ------------------------------------------------------------\n",
    "# Adding a new column (StudentGrade) and enabling schema evolution\n",
    "new_student_data = [\n",
    "    (6, \"Adeel\", 28, \"A\")\n",
    "]\n",
    "\n",
    "new_schema = StructType([\n",
    "    StructField('StudentID', IntegerType(), False),\n",
    "    StructField('StudentName', StringType(), True),\n",
    "    StructField('StudentAge', IntegerType(), True),\n",
    "    StructField('StudentGrade', StringType(), True)\n",
    "])\n",
    "\n",
    "df_new = spark.createDataFrame(new_student_data, new_schema)\n",
    "\n",
    "df_new.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_path)\n",
    "# Key Point: mergeSchema=True allows adding new columns without breaking the table\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "21_delta_advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
