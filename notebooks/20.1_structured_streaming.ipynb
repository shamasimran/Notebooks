{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 20.1_structured_streaming\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Basics of Structured Streaming in Spark\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Created streaming DataFrame from files  \n",
    "                                              Defined query with writeStream  \n",
    "                                              Used checkpointing for fault tolerance  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5799f3f4-5e86-489a-ae6d-1e8bedf30813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Root path of your Unity Catalog volume\n",
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "\n",
    "# Master folder for streaming project\n",
    "masterPath = rootPath + \"spark-streaming/\"\n",
    "\n",
    "# Delete recursively\n",
    "# dbutils.fs.rm(masterPath, recurse=True)\n",
    "\n",
    "# Define subfolders inside master\n",
    "inputPath = masterPath + \"csv_input\"\n",
    "checkpointPath = masterPath + \"checkpoints/csv_query\"\n",
    "outputPath = masterPath + \"csv_output\"\n",
    "\n",
    "# Create directories\n",
    "dbutils.fs.mkdirs(masterPath)\n",
    "dbutils.fs.mkdirs(inputPath)\n",
    "dbutils.fs.mkdirs(checkpointPath)\n",
    "dbutils.fs.mkdirs(outputPath)\n",
    "\n",
    "print(\"Master folder:\", masterPath)\n",
    "print(\"Input folder:\", inputPath)\n",
    "print(\"Checkpoint folder:\", checkpointPath)\n",
    "print(\"Output folder:\", outputPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72131d86-e0a4-4e4e-acbe-57895b5654e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0) Paths (as provided in output of previous cell)\n",
    "masterPath     = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/\"\n",
    "inputPath      = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_input\"\n",
    "checkpointPath = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/checkpoints/csv_query\"\n",
    "outputPath     = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e48d829-2751-4dfa-a063-82dcf448db97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(stream.id, stream.name, stream.status)\n",
    "\n",
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(stream.id, stream.name, stream.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a34af08-8024-41ca-a4e3-cb2736241b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Define schema for the incoming CSVs\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)  # keep for future (watermark/windows)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cff3aaa-b87c-46d5-ac7e-6a42144df10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Create the streaming DataFrame (CSV source)\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "         .option(\"header\", \"true\")   # CSV has a header row\n",
    "         .schema(schema)             # schema is required for streaming\n",
    "         .csv(inputPath)             # watches this folder for new files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b864873-0e1e-4b35-86e3-ead2b2d27813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) Start a basic console sink (no windows, no watermark)\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "             .format(\"csv\")\n",
    "             .option(\"path\", outputPath)\n",
    "             .option(\"checkpointLocation\", checkpointPath)  # enables recovery/fault tolerance\n",
    "             .outputMode(\"append\")                          # append since no aggregations\n",
    "             .trigger(once=True) # .trigger(processingTime=\"1 seconds\")\n",
    "             .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# trigger() ===>\n",
    "# Default → real-time continuous streaming.\n",
    "# Processing time → when you want controlled batch frequency.\n",
    "# Once → for testing or one-off processing.\n",
    "# AvailableNow → catch-up to current state, then stop.\n",
    "\n",
    "# outputMode(\"append\") ======>\n",
    "# Use append if: new rows keep arriving, and you don’t need to rewrite or update old results.\n",
    "# Use update if: you’re aggregating and only want changes since the last batch.\n",
    "# Use complete if: you’re aggregating and need the full table every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547adebd-457f-417c-8926-38cd0b754216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) (Optional) Stop the stream when you’re done\n",
    "# for q in spark.streams.active:\n",
    "#     q.stop()\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(stream.id, stream.name, stream.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39703b4f-3a5e-4d50-81e4-8785cfbd679d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch01_students.csv\n",
    "id,name,score,event_time\n",
    "1,Ali,85,2025-08-18 12:00:05\n",
    "2,Sara,90,2025-08-18 12:00:08\n",
    "3,Imran,70,2025-08-18 12:00:10\n",
    "\n",
    "batch02_students.csv\n",
    "id,name,score,event_time\n",
    "4,Ayesha,95,2025-08-18 12:01:02\n",
    "5,Hassan,88,2025-08-18 12:01:30"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20.1_structured_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
