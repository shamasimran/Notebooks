{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 19_pyspark_udfs\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Creating and applying PySpark UDFs\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Defined user-defined functions in PySpark  \n",
    "                                              Registered UDFs with Spark SQL  \n",
    "                                              Applied UDFs in DataFrame transformations  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13492c0-8075-441b-94df-9513df1b029e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkUDFs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a2975c-7459-43a7-b44e-b03dba1e06bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example function: categorize score\n",
    "def categorize_score(score):\n",
    "    if score >= 90:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 75:\n",
    "        return \"Good\"\n",
    "    else:\n",
    "        return \"Average\"\n",
    "\n",
    "# Register as UDF\n",
    "# Itâ€™s just a Python variable (categorize_udf) that holds a Spark UDF object.\n",
    "categorize_udf = F.udf(categorize_score, StringType())\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame([(1, 95), (2, 80), (3, 60)], [\"id\", \"score\"])\n",
    "\n",
    "# Apply UDF in DataFrame API\n",
    "df.withColumn(\"category\", categorize_udf(\"score\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82264508-faf4-4498-b8cc-d07125f2cb31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Using UDFs in Spark SQL\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Register as SQL UDF\n",
    "spark.udf.register(\"categorize_sql\", categorize_score, StringType())\n",
    "\n",
    "df.createOrReplaceTempView(\"scores\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT id, score, categorize_sql(score) AS category\n",
    "    FROM scores\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec917d1-8fe0-4baa-ab3d-5f46901a595d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Regular UDFs are slow because:\n",
    "#   - Each row moves from JVM -> Python -> JVM (serialization overhead)\n",
    "#   - Break Spark's Catalyst optimizer (no predicate pushdown, no pruning)\n",
    "# Best Practice: Always try Spark built-in functions first (F.when, F.concat, F.udf)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df.withColumn(\"category_builtin\",\n",
    "              F.when(F.col(\"score\") >= 90, \"Excellent\")\n",
    "               .when(F.col(\"score\") >= 75, \"Good\")\n",
    "               .otherwise(\"Average\")\n",
    "             ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688b9807-465b-43f4-8fbc-e3570b8e736a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Pandas UDFs (a.k.a. Vectorized UDFs)\n",
    "# - Introduced to overcome performance issues of regular UDFs\n",
    "# - Work on batches (vectorized) instead of row-by-row\n",
    "# - Use Apache Arrow for efficient data exchange\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Define Pandas UDF\n",
    "@pandas_udf(StringType())\n",
    "def categorize_pandas_udf(score: pd.Series) -> pd.Series:\n",
    "    return score.apply(lambda x: \"Excellent\" if x >= 90 else \"Good\" if x >= 75 else \"Average\")\n",
    "\n",
    "# Apply Pandas UDF\n",
    "df.withColumn(\"category_pandas\", categorize_pandas_udf(\"score\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a487a1e9-7aa8-4577-8240-3b466678f44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Best Practices:\n",
    "#   - Prefer built-in Spark SQL functions (optimized in Catalyst)\n",
    "#   - Use regular UDFs only if no built-in function exists\n",
    "#   - For heavy computation, prefer Pandas UDFs (vectorized)\n",
    "#   - Test performance: explain() can show differences\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Compare plans\n",
    "df.withColumn(\"category_builtin\",\n",
    "              F.when(F.col(\"score\") >= 90, \"Excellent\")\n",
    "               .when(F.col(\"score\") >= 75, \"Good\")\n",
    "               .otherwise(\"Average\")\n",
    "             ).explain()\n",
    "\n",
    "df.withColumn(\"category_pandas\", categorize_pandas_udf(\"score\")).explain()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "19_pyspark_udfs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
