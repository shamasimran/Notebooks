


# ğŸ“˜ Introduction to Spark Application, SparkSession, SparkContext & Reusing SparkSession

## What is a Spark Application?
A **Spark Application** is the complete program you run on Spark.  
It usually contains:

- **Driver program** â€” runs your main logic, creates `SparkSession` / `SparkContext`, and coordinates work.
- **Executors** â€” worker processes that execute tasks and hold data.
- **Cluster manager** â€” allocates resources (YARN, Kubernetes, Mesos, or Spark Standalone).

ğŸ‘‰ Think of a Spark Application as the entire job (your Python/Scala script or notebook) submitted to Spark.

---

## What is SparkContext?
- `SparkContext` is the **low-level connection** to the Spark cluster.
- Responsibilities:
  - Talks to the cluster manager.
  - Requests executors and schedules tasks.
  - Manages distributed RDDs and execution.

Code examples:

```python
# Old style (before Spark 2.0)
from pyspark import SparkContext
sc = SparkContext(appName="MyApp")

# Modern way (via SparkSession)
sc = spark.sparkContext


## 1ï¸âƒ£ What is SparkSession?
- The **entry point** to using Spark in Python.  
- Provides an interface to work with:
  - **DataFrames**
  - **SQL queries**
  - **RDDs**
- SparkSession internally manages the **SparkContext** (low-level) and SQL functionality.

---

## 2ï¸âƒ£ Creating a SparkSession

```python
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("DatapurProgram").getOrCreate()
```

âœ… **Explanation**:  
- `SparkSession.builder` â†’ used to configure your Spark application.  
- `.appName("DatapurProgram")` â†’ assigns a name to the Spark job (visible in the Spark UI).  
- `.getOrCreate()` â†’ creates a new session if one does not exist, otherwise reuses the existing session.  

ğŸ” **Result**: You now have a `spark` object to interact with Spark.  

# Spark Application, SparkSession, and SparkContext â€“ Diagram

```
Spark Application
â”‚
â”œâ”€â”€ Driver Program
â”‚   â”‚
â”‚   â”œâ”€â”€ SparkSession (High-level entry point for users)
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ SparkContext (Connection to cluster)
â”‚   â”‚               â”‚
â”‚   â”‚               â”œâ”€â”€ Schedules tasks
â”‚   â”‚               â”œâ”€â”€ Talks to Cluster Manager
â”‚   â”‚               â”‚       (YARN / Kubernetes / Standalone / Mesos)
â”‚   â”‚               â””â”€â”€ Manages RDDs and executors
â”‚   â”‚
â”‚   â””â”€â”€ User Code
â”‚           (DataFrame, SQL, RDD APIs)
â”‚
â””â”€â”€ Executors (Worker processes)
        â”‚
        â”œâ”€â”€ Run tasks from SparkContext
        â””â”€â”€ Store data (cache, shuffle, intermediate results)
```

## 3ï¸âƒ£ Exploring Spark Documentation

```python
help(spark)
```

- Displays built-in documentation for the `spark` object.  
- Useful for discovering methods and properties of the SparkSession.  

```python
help(spark.createDataFrame)
```
- Shows documentation for the `createDataFrame` method.  
- This method is used to convert lists, pandas DataFrames, or RDDs into Spark DataFrames.

---

## 4ï¸âƒ£ Exploring PySpark Modules

```python
import pyspark.sql.types
help(pyspark.sql)
#help(pyspark.sql.types)
```

- `pyspark.sql` â†’ main module for Spark SQL features.  
- `pyspark.sql.types` â†’ defines data types used in DataFrames.  
- `help()` function lets you browse available methods and documentation.

---

## 5ï¸âƒ£ Defining a Schema

```python
import pyspark.sql.types

schema = pyspark.sql.types.StructType([
    pyspark.sql.types.StructField("id", pyspark.sql.types.IntegerType(), True),
    pyspark.sql.types.StructField("name", pyspark.sql.types.StringType(), True)
])

display(schema)
```

âœ… **Explanation**:  
- `StructType` = a schema definition (like a table structure).  
- `StructField("id", IntegerType(), True)` â†’ a column named `id`, type integer, nullable.  
- `StructField("name", StringType(), True)` â†’ a column named `name`, type string, nullable.  
- `display(schema)` â†’ shows schema details in the notebook interface.

---

## 6ï¸âƒ£ Why Define a Schema?
- Ensures data types are **consistent**.  
- Prevents Spark from incorrectly inferring types.  
- Improves **performance** (Spark doesnâ€™t need to scan the file to guess schema).  
- Useful for production ETL pipelines where data quality matters.  

---

## ğŸ“ Summary
- A **SparkSession** is your entry point to Spark in PySpark.  
- Use `.builder.appName().getOrCreate()` to start/reuse a session.  
- Explore objects using `help()` for quick documentation lookup.  
- Define schemas with `StructType` and `StructField` to control data structure.  
