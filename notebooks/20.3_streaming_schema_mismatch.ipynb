{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 20.3_streaming_bad_data\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Handling bad or corrupt data in streaming pipelines\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Simulated bad records in streaming input  \n",
    "                                              handled schema mismatch  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5799f3f4-5e86-489a-ae6d-1e8bedf30813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PySpark Structured Streaming - Handling Bad CSV Records\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Spark Session\n",
    "# ------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CSV_Streaming_Error_Handling\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Folder Paths (your provided Unity Catalog paths)\n",
    "# ------------------------------------------------------------\n",
    "inputPath       = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_input\"\n",
    "checkpointPath  = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/checkpoints/csv_query\"\n",
    "outputPath      = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_output\"\n",
    "badRecordsPath  = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_bad_records\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Define Schema (expected columns)\n",
    "# ------------------------------------------------------------\n",
    "# NOTE: we add `_corrupt_record` column to capture invalid rows.\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),         # should be integer\n",
    "    StructField(\"name\", StringType(), True),        # string field\n",
    "    StructField(\"score\", IntegerType(), True),      # should be integer\n",
    "    StructField(\"event_time\", TimestampType(), True), # event time (for future)\n",
    "    StructField(\"_corrupt_record\", StringType(), True)  # keeps the bad row\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Create Streaming DataFrame with Error Handling\n",
    "# ------------------------------------------------------------\n",
    "# mode = PERMISSIVE => put invalid rows into _corrupt_record\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "         .option(\"header\", \"true\")                           # CSV has header\n",
    "         .schema(schema)                                     # enforce schema\n",
    "         .option(\"mode\", \"PERMISSIVE\")                       # keep bad rows\n",
    "         .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "         .csv(inputPath)                                     # folder to watch\n",
    ")\n",
    "\n",
    "# option(\"mode\" ============>\n",
    "# PERMISSIVE (default) → Keeps all rows; bad rows go into _corrupt_record column.\n",
    "# DROPMALFORMED → Skips bad rows completely (they are dropped).\n",
    "# FAILFAST → Stops the job immediately when a bad row is found.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Separate Good vs Bad Records\n",
    "# ------------------------------------------------------------\n",
    "valid_rows = df_stream.filter(df_stream[\"_corrupt_record\"].isNull())\n",
    "bad_rows   = df_stream.filter(df_stream[\"_corrupt_record\"].isNotNull())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Write Valid Records to Output Folder\n",
    "# ------------------------------------------------------------\n",
    "valid_query = (\n",
    "    valid_rows.writeStream\n",
    "              .format(\"csv\")                                 # write CSVs\n",
    "              .option(\"path\", outputPath)                    # output folder\n",
    "              .option(\"checkpointLocation\", checkpointPath + \"/valid\") # unique checkpoint\n",
    "              .outputMode(\"append\")                          # append = new rows only\n",
    "              .start()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Write Bad Records to Quarantine Folder\n",
    "# ------------------------------------------------------------\n",
    "bad_query = (\n",
    "    bad_rows.writeStream\n",
    "            .format(\"csv\")                                   # store bad rows separately\n",
    "            .option(\"path\", badRecordsPath)\n",
    "            .option(\"checkpointLocation\", checkpointPath + \"/bad\") # unique checkpoint\n",
    "            .outputMode(\"append\")\n",
    "            .start()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Wait for Streams to Finish\n",
    "# ------------------------------------------------------------\n",
    "# This keeps your job running until you stop it manually\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f074663f-37fe-4b84-ad65-d906c412e82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Valid_data.csv\n",
    "id,name,score,event_time\n",
    "1,Ali,85,2025-08-18 10:00:00\n",
    "2,Sara,90,2025-08-18 10:05:00\n",
    "3,Omar,75,2025-08-18 10:10:00\n",
    "\n",
    "invalidd_ata.csv\n",
    "id,name,score,event_time,extra\n",
    "4,Ayesha,88,2025-08-18 10:15:00,unexpected"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20.3_streaming_schema_mismatch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
