{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 18_dataframe_optimizations\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Optimizing DataFrame performance\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Demonstrated caching and persistence  \n",
    "                                              Used repartition and coalesce  \n",
    "                                              Analyzed execution plan with explain  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c64228-4dad-4105-aa4b-083274f06250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameOptimizations\").getOrCreate()\n",
    "\n",
    "# Sample DataFrames\n",
    "df_large = spark.range(0, 1_000_000).withColumn(\"value\", (F.rand() * 100).cast(\"int\"))\n",
    "df_small = spark.createDataFrame([(i, f\"cat_{i}\") for i in range(100)], [\"id\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f257c13c-88fa-4b08-8343-e2c5636dc3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                        # ---------------------------------------------------------\n",
    "                                        # 1. Repartition vs Coalesce\n",
    "                                        # ---------------------------------------------------------\n",
    "# Repartition increases or decreases partitions (full shuffle)\n",
    "df_repart = df_large.repartition(10)\n",
    "\n",
    "# print(df_repart.rdd.glom().collect()) \n",
    "count_df_repart = \"You cannot use RDD APIs (like .rdd.getNumPartitions()) on Databricks serverless compute\"  #df_repart.rdd.getNumPartitions() \n",
    "print(\"Number of partitions after repartition:\", count_df_repart)\n",
    "\n",
    "# Coalesce only decreases partitions (avoids full shuffle if possible)\n",
    "df_coalesce = df_large.coalesce(2)\n",
    "count_df_coalesce = 0 # df_coalesce.rdd.getNumPartitions()\n",
    "print(\"Number of partitions after coalesce:\", count_df_coalesce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1efe3d8-ab01-46c4-b972-8adba5ff8e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                        # ---------------------------------------------------------\n",
    "                                        # 2. Caching and Persistence\n",
    "                                        # ---------------------------------------------------------\n",
    "# Cache: stores the DataFrame in memory (default MEMORY_AND_DISK)\n",
    "df_cached = df_large.cache()\n",
    "print(\"First count triggers caching:\", df_cached.count())  # first action\n",
    "print(\"Second count reads from cache:\", df_cached.count())\n",
    "\n",
    "# Persist: allows specifying storage levels\n",
    "from pyspark import StorageLevel\n",
    "df_persist = df_large.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ae509a-8833-4236-a770-44a274f64bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. Broadcast Join Optimization\n",
    "# ---------------------------------------------------------\n",
    "# Broadcast small DataFrame to avoid shuffle join\n",
    "df_join = df_large.join(F.broadcast(df_small), df_large.value == df_small.id, \"inner\")\n",
    "df_join.explain()  # Check execution plan for BroadcastHashJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f4196f-91e3-4ec0-94ce-2d01e9ed43ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. Handling Data Skew\n",
    "# ---------------------------------------------------------\n",
    "# Simulate skew: most rows have value = 1\n",
    "df_skewed = df_large.withColumn(\"key\", F.when(F.rand() < 0.9, 1).otherwise(F.col(\"value\")))\n",
    "\n",
    "# Skew mitigation: add a \"salt\" key to spread data\n",
    "df_skewed_salted = df_skewed.withColumn(\"salt\", (F.rand() * 10).cast(\"int\"))\n",
    "df_small_salted = df_small.withColumn(\"salt\", F.expr(\"explode(sequence(0,9))\"))\n",
    "\n",
    "df_salted_join = df_skewed_salted.join(\n",
    "    df_small_salted,\n",
    "    (df_skewed_salted.key == df_small_salted.id) & (df_skewed_salted.salt == df_small_salted.salt),\n",
    "    \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef00b7fd-78ce-48c0-ae90-eea706b43c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. Measuring Execution Plans\n",
    "# ---------------------------------------------------------\n",
    "# Explain gives logical and physical execution plans\n",
    "df_join.explain(True)  # Detailed execution plan\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "18_dataframe_optimizations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
