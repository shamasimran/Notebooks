{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PySpark Structured Streaming - Joins (Stream–Stream & Stream–Static)\n",
    "# ============================================================\n",
    "\n",
    "# Key Concepts:\n",
    "# 1) Stream–Stream join: joins two streaming DataFrames on a key; requires watermark for state management.\n",
    "# 2) Stream–Static join: joins a streaming DataFrame with a static DataFrame (lookup table); no watermark needed.\n",
    "# 3) Watermark: used in stream–stream joins to handle late data and avoid unbounded state.\n",
    "# 4) Output mode: 'append', 'update', or 'complete' depending on aggregation/join type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5799f3f4-5e86-489a-ae6d-1e8bedf30813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Spark Session\n",
    "# ------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Stream_Joins\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Folder Paths for Streaming Data\n",
    "# ------------------------------------------------------------\n",
    "inputPath1       = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_input_stream1\"\n",
    "inputPath2       = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_input_stream2\"\n",
    "checkpointPath   = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/checkpoints/joins\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Define Schema for Streams\n",
    "# ------------------------------------------------------------\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Create Streaming DataFrames\n",
    "# ------------------------------------------------------------\n",
    "df_stream1 = spark.readStream.option(\"header\", \"true\").schema(schema1).csv(inputPath1)\n",
    "df_stream2 = spark.readStream.option(\"header\", \"true\").schema(schema2).csv(inputPath2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Stream–Stream Join\n",
    "# ------------------------------------------------------------\n",
    "# Apply watermark to both streams to manage state for late data\n",
    "df_stream1_watermarked = df_stream1.withWatermark(\"event_time\", \"10 minutes\")\n",
    "df_stream2_watermarked = df_stream2.withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "stream_stream_join = df_stream1_watermarked.join(\n",
    "    df_stream2_watermarked,\n",
    "    expr(\"\"\"\n",
    "        df_stream1.id = df_stream2.id AND\n",
    "        df_stream1.event_time >= df_stream2.event_time AND\n",
    "        df_stream1.event_time <= df_stream2.event_time + interval 5 minutes\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "stream_stream_query = (\n",
    "    stream_stream_join.writeStream\n",
    "                      .format(\"console\")\n",
    "                      .option(\"checkpointLocation\", checkpointPath + \"/stream_stream\")\n",
    "                      .outputMode(\"append\")  # append mode works for stream–stream\n",
    "                      .start()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Stream–Static Join\n",
    "# ------------------------------------------------------------\n",
    "# Static DataFrame (lookup table)\n",
    "static_data = [\n",
    "    (1, \"IT\"),\n",
    "    (2, \"HR\"),\n",
    "    (3, \"Finance\"),\n",
    "    (4, \"Marketing\")\n",
    "]\n",
    "df_static = spark.createDataFrame(static_data, [\"id\", \"department\"])\n",
    "\n",
    "stream_static_join = df_stream1.join(df_static, on=\"id\", how=\"left\")\n",
    "\n",
    "stream_static_query = (\n",
    "    stream_static_join.writeStream\n",
    "                       .format(\"console\")\n",
    "                       .option(\"checkpointLocation\", checkpointPath + \"/stream_static\")\n",
    "                       .outputMode(\"append\")\n",
    "                       .start()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Wait for Streams\n",
    "# ------------------------------------------------------------\n",
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b7c786-6c76-4558-992d-ad3e79a45898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "id,name,score,event_time\n",
    "1,Ahsan,85,2025-08-18 10:00:00\n",
    "2,Sana,92,2025-08-18 10:01:00\n",
    "3,Ali,78,2025-08-18 10:02:00\n",
    "4,Ahsan,88,2025-08-18 10:03:00\n",
    "5,Sana,95,2025-08-18 10:04:00\n",
    "6,Ali,82,2025-08-18 10:06:00\n",
    "7,Ahsan,90,2025-08-18 10:07:00\n",
    "8,Sana,87,2025-08-18 10:09:00\n",
    "9,Ali,91,2025-08-18 10:10:00\n",
    "10,Ahsan,80,2025-08-18 09:50:00   # Late, within 10-min watermark\n",
    "11,Sana,85,2025-08-18 09:40:00    # Too late, will be dropped by watermark\n",
    "12,Ali,88,2025-08-18 10:12:00\n",
    "13,Ahsan,92,2025-08-18 10:14:00\n",
    "14,Sana,90,2025-08-18 10:15:00\n",
    "15,Ali,83,2025-08-18 10:18:00"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20.6_joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
