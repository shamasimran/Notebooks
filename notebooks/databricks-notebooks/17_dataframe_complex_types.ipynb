{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 17_dataframe_complex_types\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Working with complex data types (arrays, maps, structs)\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Created DataFrames with arrays and structs  \n",
    "                                              Accessed nested fields  \n",
    "                                              Applied explode on arrays  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02217a0f-28e2-4c8f-be0b-69a86a020da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2517241-7610-4862-a3ee-f024a46ddd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                        # ===============================\n",
    "                                                        # 1. ARRAYS\n",
    "                                                        # ===============================\n",
    "\n",
    "\n",
    "# Create a sample DataFrame with an array column\n",
    "df_array = spark.createDataFrame([\n",
    "    (1, [\"python\", \"spark\", \"sql\", \"pandas\", \"scala\"]),\n",
    "    (2, [\"azure\", \"databricks\"])\n",
    "], [\"id\", \"skills\"])\n",
    "\n",
    "df_array.show(truncate=False)   # Display arrays as-is\n",
    "\n",
    "# Explode array into multiple rows (1 row per skill)\n",
    "df_array_exploded = df_array.withColumn(\"skill\", F.explode(\"skills\"))\n",
    "df_array_exploded.show()\n",
    "\n",
    "# Use array functions: size (count elements), array_contains (search element)\n",
    "df_array.select(\n",
    "    \"id\",\n",
    "    F.size(\"skills\").alias(\"skill_count\"),               # count elements in array\n",
    "    F.array_contains(\"skills\", \"spark\").alias(\"knows_spark\")  # check if \"spark\" is present\n",
    ").show() # Each column is truncated to 20 characters (longer text gets cut with ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78341b2d-9305-4cfb-a3e8-30d77fe8a611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                        # ===============================\n",
    "                                                        # 2. STRUCTS\n",
    "                                                        # ===============================\n",
    "\n",
    "# Create a sample DataFrame with tuples that we will convert to struct\n",
    "df_struct = spark.createDataFrame([\n",
    "    (1, (\"John\", \"Doe\")),\n",
    "    (2, (\"Jane\", \"Smith\"))\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Convert tuple into a struct with named fields: first, last\n",
    "df_struct = df_struct.withColumn(\"name_struct\", \n",
    "    F.struct(F.col(\"name._1\").alias(\"first\"), F.col(\"name._2\").alias(\"last\"))\n",
    ")\n",
    "\n",
    "# Access struct fields using dot notation\n",
    "df_struct.select(\"id\", \"name_struct.first\", \"name_struct.last\").show()\n",
    "\n",
    "# OR using dot notation in string\n",
    "df_struct.filter(\"name_struct.last = 'Doe'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ab03f5-04b7-4f9a-882c-9a56fcdc844e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                                                                    # ===============================\n",
    "                                                                    # 3. JSON\n",
    "                                                                    # ===============================\n",
    "\n",
    "# Create a sample DataFrame with JSON string column\n",
    "df_json = spark.createDataFrame([\n",
    "    (1, '{\"city\":\"New York\",\"zip\":10001}'),\n",
    "    (2, '{\"city\":\"LA\",\"zip\":90001}')\n",
    "], [\"id\", \"address_json\"])\n",
    "\n",
    "# Parse JSON string into a struct with schema (city, zip)\n",
    "df_parsed = df_json.withColumn(\"address\", \n",
    "    F.from_json(\"address_json\", \"city STRING, zip INT\")\n",
    ")\n",
    "\n",
    "# Access fields inside the parsed struct\n",
    "df_parsed.select(\"id\", \"address.city\", \"address.zip\").show()\n",
    "\n",
    "# Convert struct back to JSON string\n",
    "df_tojson = df_parsed.withColumn(\"json_again\", F.to_json(\"address\"))\n",
    "df_tojson.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17_dataframe_complex_types",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
