{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "844088d7-a23f-480e-9cf4-0896e1f0cc59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 04.1_dataframe_csv_and_parquet\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Reading and writing DataFrames in CSV and Parquet formats\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Read CSV files with header and schema  \n",
    "                                              Saved DataFrames to Parquet  \n",
    "                                              Compared CSV vs Parquet performance  \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here are comments added to your notebook code cells to explain each step:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DatapurProgram\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root path for data files\n",
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "filepath = rootPath + \"ncr_ride_bookings.csv\"\n",
    "\n",
    "try:\n",
    "    # Read CSV file with header and infer schema automatically\n",
    "    df_csv = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "    print(\"CSV file contents:\")\n",
    "    display(df_csv)\n",
    "except Exception as e:\n",
    "    # Handle missing file scenario\n",
    "    print(\"Upload 'ncr_ride_bookings.csv' to run this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show VS Display Function\n",
    "| Feature              | `show()` (Spark)           | `display()` (Databricks)          |\n",
    "|-----------------------|-----------------------------|-----------------------------------|\n",
    "| Environment           | Works in any Spark session | Only in Databricks notebooks      |\n",
    "| Output                | Text table (console-style) | Interactive UI table              |\n",
    "| Row limit             | 20 by default (configurable) | 1000 by default                   |\n",
    "| Interactivity         | ❌ None                     | ✅ Sorting, filtering, exporting, charts |\n",
    "| Visualization support | ❌ No                       | ✅ Yes (built-in visualizations)   |\n",
    "| Typical use case      | Quick inspection, debugging | Data exploration & visualization  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "filepath = rootPath + \"ncr_ride_bookings.csv\"\n",
    "\n",
    "# df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(filepath)\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .csv(filepath)\n",
    "\n",
    "# df.printSchema()\n",
    "df.show(10)\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types for schema definition\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define schema for student DataFrame\n",
    "student_schema = StructType([\n",
    "    StructField('StudentID', IntegerType(), False),\n",
    "    StructField('StudentName', StringType(), True),\n",
    "    StructField('StudentAge', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample student data\n",
    "student_data = [\n",
    "        (1, \"Alice\", 34), \n",
    "        (2, \"Bob\", 45), \n",
    "        (3, \"Charlie\", 29),\n",
    "        (4, \"Shamas\", 40)\n",
    "        ]\n",
    "\n",
    "# Create DataFrame using schema and data\n",
    "df_student = spark.createDataFrame(student_data, student_schema)\n",
    "df_student.show()  # Display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for course DataFrame\n",
    "course_schema = StructType([\n",
    "    StructField('CourseID', IntegerType(), False),\n",
    "    StructField('CourseName', StringType(), True),\n",
    "    StructField('CourseTitle', StringType(), True),\n",
    "])\n",
    "\n",
    "# Sample course data\n",
    "course_data = [\n",
    "        (1, \"Physics\", \"1111\"), \n",
    "        (2, \"Chemistry\", \"2222\"), \n",
    "        (3, \"English\", \"3333\"),\n",
    "        (4, \"Computer Science\", \"4444\")\n",
    "        ]\n",
    "\n",
    "# Create DataFrame using schema and data\n",
    "df_course = spark.createDataFrame(course_data, course_schema)\n",
    "df_course.show()  # Display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving parquet files\n",
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "studentFilePath = rootPath + \"student\"\n",
    "courseFilePath = FilePath = rootPath + \"course\"\n",
    "\n",
    "# Write student DataFrame to Parquet format\n",
    "df_student.write.mode(\"overwrite\").parquet(studentFilePath)\n",
    "print(\"Parquet file written to \" + studentFilePath)\n",
    "\n",
    "# Write course DataFrame to Parquet format\n",
    "df_course.write.mode(\"overwrite\").parquet(courseFilePath)\n",
    "print(\"Parquet file written to \" + courseFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file and display its contents\n",
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "filepath = rootPath + \"student\"\n",
    "try:\n",
    "    # Read Parquet file (header and inferSchema not needed for parquet)\n",
    "    df_parquet = spark.read.parquet(filepath, header=True, inferSchema=True)\n",
    "    print(\"parquet file contents:\")\n",
    "    display(df_parquet)\n",
    "except Exception as e:\n",
    "    print(\"Upload 'student.parquet' to run this step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display course and student DataFrames\n",
    "df_course.show()\n",
    "df_student.show()\n",
    "\n",
    "# Get the number of partitions for each DataFrame\n",
    "# num_partitions_course = df_course.rdd.getNumPartitions()\n",
    "# num_partitions_student = df_student.rdd.getNumPartitions()\n",
    "\n",
    "# print(f\"Number of partitions in course DataFrame: {num_partitions_course}\")\n",
    "# print(f\"Number of partitions in student DataFrame: {num_partitions_student}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.parquet(\n",
    "#     \"Files/client_output_data/parquet/student.parquet/part-00000.snappy.parquet\",\n",
    "#     \"Files/client_output_data/parquet/student.parquet/part-00001.snappy.parquet\"\n",
    "# )\n",
    "\n",
    "# df = spark.read.parquet(\"Files/client_output_data/parquet/student/part-*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving CSV files\n",
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/\"\n",
    "studentFolderPath = rootPath + \"student\"\n",
    "\n",
    "df_student.coalesce(1) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet(studentFolderPath)\n",
    "print(\"file written to \" + studentFolderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91eabdb6-41f2-4e2d-b136-069c2f32c1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "print(\"Original:\", df_student.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce\n",
    "df_coal = df_student.coalesce(2)\n",
    "print(\"Coalesce(2):\", df_coal.rdd.getNumPartitions())\n",
    "\n",
    "# Repartition\n",
    "df_repart = df_student.repartition(2)\n",
    "print(\"Repartition(2):\", df_repart.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Repartition\n",
    "df_repart = df_student.repartition(20)\n",
    "print(\"Repartition(20):\", df_repart.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark Quick Concepts\n",
    "\n",
    "| Concept       | One-liner Explanation |\n",
    "|---------------|---------------------|\n",
    "| coalesce(n)   | Reduces the number of partitions without shuffle (fast, may be uneven). |\n",
    "| repartition(n)| Creates exactly n partitions with shuffle (balanced, more expensive). |\n",
    "| Data Skew     | Some partitions have much more data than others, causing slow tasks. |\n",
    "| Shuffle       | Moves data across partitions/executors to align for operations like join/groupBy. |\n",
    "\n",
    "\n",
    "# Difference: `coalesce()` vs `repartition()`\n",
    "\n",
    "| Feature             | `coalesce(n)` | `repartition(n)` |\n",
    "|---------------------|---------------|------------------|\n",
    "| Shuffle operation   | ❌ No shuffle when reducing partitions <br> ✅ Shuffle if increasing | ✅ Always triggers shuffle |\n",
    "| Increase partitions | ⚠️ Possible but inefficient | ✅ Efficient and balanced |\n",
    "| Decrease partitions | ✅ Fast (just merges partitions) <br> ⚠️ May cause uneven distribution | ✅ Balanced distribution (reshuffle) |\n",
    "| Performance         | Faster, less expensive | More expensive due to shuffle |\n",
    "| Data distribution   | Can be skewed/uneven | Evenly distributed |\n",
    "| Typical use case    | Write fewer output files quickly (e.g., `coalesce(1)`) | Prepare balanced data for joins, aggregations, or large writes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f772c4-0733-4257-b41e-765594a47686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema_person = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data_person = [\n",
    "    (1, \"Ali\",   \"PK\", 2024, 1),\n",
    "    (2, \"Ahmed\", \"PK\", 2024, 2),\n",
    "    (3, \"Sara\",  \"PK\", 2025, 1),\n",
    "    (4, \"John\",  \"US\", 2024, 1),\n",
    "    (5, \"Maria\", \"US\", 2025, 2),\n",
    "    (6, \"Chen\",  \"CN\", 2024, 2)\n",
    "]\n",
    "\n",
    "df_person = spark.createDataFrame(data_person, schema=schema_person)\n",
    "\n",
    "df_person.show()\n",
    "df_person.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea28974-92cd-42c4-9887-cbcb1efeda7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rootPath = \"/Volumes/datapurcatalog/default/datapurvolume/partitioned_person\"\n",
    "output_path = rootPath + \"student\"\n",
    "df_person.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\", \"year\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b3a30c-220f-4790-b628-dfd7d95af7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(output_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8351ae-234c-4626-b980-d7a4a5ff49a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(\"country = 'PK'\").show()\n",
    "df.filter(\"country = 'US' AND year = 2025\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Skew and Shuffle: Good vs Bad\n",
    "\n",
    "## 🔹 Data Skew\n",
    "### ✅ Good\n",
    "- None inherently — skew is usually **bad**.  \n",
    "- The only \"good\" aspect is that **identifying skew** can help optimize partitioning and improve performance.  \n",
    "\n",
    "### ❌ Bad\n",
    "- Some partitions get **huge data**, while others have very little → tasks are **unbalanced**.  \n",
    "- Causes **stragglers** (a few slow tasks delay the entire job).  \n",
    "- Leads to **memory pressure**, sometimes OOM errors.  \n",
    "- Inefficient resource utilization (some executors idle while others overloaded).  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Shuffle\n",
    "### ✅ Good\n",
    "- Enables **wide transformations** like `groupBy`, `reduceByKey`, `join`.  \n",
    "- Redistributes data across partitions → ensures **correctness** of operations.  \n",
    "- Necessary for **load balancing** in some cases (e.g., after skew fix).  \n",
    "- Enables **parallelism** across nodes.  \n",
    "\n",
    "### ❌ Bad\n",
    "- **Expensive** operation → involves disk I/O, network I/O, and serialization.  \n",
    "- Can generate **large intermediate files**.  \n",
    "- Prone to **shuffle spill** → increases job runtime.  \n",
    "- More shuffles → **slower job**, higher cluster cost.  \n",
    "\n",
    "---\n",
    "\n",
    "👉 **Rule of Thumb**:  \n",
    "- **Data skew = always harmful** → needs fixing (via `repartition`, `salting`, `map-side combine`, etc.).  \n",
    "- **Shuffle = necessary evil** → bad for performance, but sometimes essential for correctness. Minimize, but don’t avoid at cost of wrong results.  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8218812668408036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.1_dataframe_csv_and_parquet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
