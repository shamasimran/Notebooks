{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "844088d7-a23f-480e-9cf4-0896e1f0cc59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 08_dataframe_union\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Combining DataFrames using union operations\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Demonstrated union of two DataFrames  \n",
    "                                              Showed schema alignment requirements  \n",
    "                                              Highlighted union vs unionByName  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de975541-a853-44e0-933f-06ee263b604b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DatapurProgram\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41653eda-9914-4ec0-9feb-e1ebac7f625a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "student_ALevel_schema = StructType([\n",
    "    StructField('ID', IntegerType(), False),\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "student_ALevel_data = [        \n",
    "        (1, \"Muhammad\", 17), \n",
    "        (2, \"Abdul Muqeet\", 16),\n",
    "        (3, \"Musa\", 15)         \n",
    "        ]\n",
    "\n",
    "df_student_ALevel = spark.createDataFrame(student_ALevel_data, student_ALevel_schema)\n",
    "df_student_ALevel.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f230a0-4e33-420e-83bb-708250927cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "student_OLevel_schema = StructType([\n",
    "    StructField('ID', IntegerType(), False),\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "student_OLevel_data = [ \n",
    "        (1, \"Muhammad\", 17),        \n",
    "        (2, \"Irfan\", 18), \n",
    "        (3, \"Shamas\", 40), \n",
    "        (4, \"Imran\", 38)\n",
    "        ]\n",
    "\n",
    "df_student_OLevel = spark.createDataFrame(student_OLevel_data, student_OLevel_schema)\n",
    "df_student_OLevel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6adc2b-5ad8-4ae3-928d-f8f34e2c3976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union = df_student_ALevel.union(df_student_OLevel)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ab0cc9-42ca-4071-8fef-d747036f8eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(1, \"Imran\")],\n",
    "    [\"id\", \"name\"]\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(\"Shamas\", 2)],\n",
    "    [\"name\", \"id\"]   # Notice reversed order\n",
    ")\n",
    "\n",
    "df_union = df1.unionByName(df2)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f193c452-2c15-4e05-beaf-84fbacdd27db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(1, \"Shamas\")],\n",
    "    [\"id\", \"name\"]\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(2, \"Imran\", 25)],\n",
    "    [\"id\", \"name\", \"age\"]\n",
    ")\n",
    "#df_union = df1.unionByName(df2) Error due to mismatching number of columns\n",
    "df_union = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5989b18-36da-4978-95b5-aa78be3493dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Shamas\")], [\"id\", \"name\"])     # id = int\n",
    "df2 = spark.createDataFrame([(\"2\", \"Imran\")], [\"id\", \"name\"])  # id = string\n",
    "\n",
    "df_union = df1.unionByName(df2)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a13441-3a9e-4e82-a408-4ce6b023dca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert DOB string to date object for df1\n",
    "df1 = spark.createDataFrame(\n",
    "    [\n",
    "        (\n",
    "            1,\n",
    "            \"Shamas\",\n",
    "            datetime.strptime(\"1986-11-10\", \"%Y-%m-%d\").date()\n",
    "        )\n",
    "    ],\n",
    "    schema=StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"DOB\", DateType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# df2 has DOB as StringType\n",
    "df2 = spark.createDataFrame(\n",
    "    [(2, \"Imran\", \"19980721\")], # Also try with 1998-07-21 and comment type conversion\n",
    "    [\"id\", \"name\", \"DOB\"]\n",
    ")\n",
    "\n",
    "# Cast df1.DOB to DateType to match df2\n",
    "df1 = df1.withColumn(\"DOB\",df1[\"DOB\"].cast(StringType()))\n",
    "\n",
    "df_union = df1.unionByName(df2)\n",
    "display(df_union)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8218812668408036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "08_dataframe_union",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
