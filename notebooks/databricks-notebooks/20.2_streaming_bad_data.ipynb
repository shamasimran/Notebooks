{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea694c53-d8f9-4261-bbd5-35117e95910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Name:** 20.2_streaming_bad_data\n",
    "- **Author:** Shamas Imran\n",
    "- **Desciption:** Handling schema mismatch in streaming data\n",
    "- **Date:** 19-Aug-2025\n",
    "<!--\n",
    "REVISION HISTORY\n",
    "Version          Date        Author           Desciption\n",
    "01           19-Aug-2025   Shamas Imran       Demonstrated bad-data handling                                                  \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5799f3f4-5e86-489a-ae6d-1e8bedf30813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) Paths (your folders)\n",
    "# ------------------------------\n",
    "inputPath = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/csv_input\"\n",
    "checkpointPath = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/checkpoints/csv_query\"\n",
    "badRecordsPath = \"/Volumes/datapurcatalog/default/datapurvolume/spark-streaming/bad_records\"\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Define expected schema\n",
    "# ------------------------------\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Create streaming DataFrame\n",
    "# ------------------------------\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "         .option(\"header\", \"true\")                 # CSV header\n",
    "         .option(\"badRecordsPath\", badRecordsPath) # Capture corrupt records\n",
    "         .schema(schema)                           # Enforce schema\n",
    "         .csv(inputPath)                           # Source folder\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Optional: Validate data\n",
    "# ------------------------------\n",
    "# Example: filter out rows missing critical fields\n",
    "df_valid = df_stream.filter(\"id IS NOT NULL AND score IS NOT NULL\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Write to console sink\n",
    "# ------------------------------\n",
    "query = (\n",
    "    df_valid.writeStream\n",
    "            .format(\"console\")                     # Print rows to console\n",
    "            .option(\"checkpointLocation\", checkpointPath)  \n",
    "            .outputMode(\"append\")                  # Append-only (no aggregations yet)\n",
    "            .trigger(once=True)                    # Run once and stop\n",
    "            .start()\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Wait for completion\n",
    "# ------------------------------\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4478d46-a453-4d72-b762-75fa37120b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "id,name,score,event_time\n",
    "1,John,85,2025-08-18 10:00:00\n",
    "2,Jane,92,2025-08-18 10:05:00\n",
    "3,Bob,78,2025-08-18 10:10:00\n",
    ",MissingID,88,2025-08-18 10:15:00           # invalid: id is NULL\n",
    "4,Alice,abc,2025-08-18 10:20:00              # invalid: score is not integer\n",
    "5,Charlie,95,not_a_timestamp                  # invalid: event_time malformed\n",
    "6,,88,2025-08-18 10:30:00                     # invalid: name is NULL (optional)\n",
    "7,David,82,2025-08-18 10:35:00\n",
    "8,Eva,90,2025-08-18 10:40:00"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20.2_streaming_bad_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
