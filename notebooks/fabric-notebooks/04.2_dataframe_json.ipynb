{"cells":[{"cell_type":"markdown","source":["- Name: 04.2_dataframe_json\n","- Author: Shamas Imran\n","- Desciption: Working with JSON files using PySpark DataFrames\n","- Date: 10-Oct-2025"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b1d8d1c-78a9-482c-a2dd-d5dc1659a3c1"},{"cell_type":"code","source":["# JSON string with slight schema differences (schema drift)\n","json_string_v1 = \"\"\"\n","[\n","  {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"city\": \"Lahore\"},\n","  {\"id\": 2, \"name\": \"Bob\", \"city\": \"Karachi\"},\n","  {\"id\": 3, \"name\": \"Charlie\", \"age\": \"30\", \"country\": \"Pakistan\"}\n","]\n","\"\"\"\n","\n","############# Basic Parsing\n","import json\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n","# Parse the JSON string into Python objects (list of dicts)\n","data_v1 = json.loads(json_string_v1)\n","\n","# Convert to a PySpark DataFrame\n","df_without_schema_v1 = spark.createDataFrame(data_v1)\n","\n","# Display the DataFrame\n","display(df_without_schema_v1)\n","df_without_schema_v1.printSchema()\n","print(df_without_schema_v1.schema)\n","\n","# Show the content\n","df_without_schema_v1.show(truncate=False)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3a4017b6-b65e-4dcc-901d-3fc498f207b8"},{"cell_type":"code","source":["############# Explicit Schema Definition\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","\n","# Explicit schema to handle drift safely\n","schema_v1 = StructType([\n","    StructField(\"id\", IntegerType(), True),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"age\", StringType(), True),   # keep as string (for mixed types)\n","    StructField(\"city\", StringType(), True),\n","    StructField(\"country\", StringType(), True)\n","])\n","\n","df_with_schema_v1 = spark.createDataFrame(data, schema=schema_v1)\n","df_with_schema_v1.show()\n","print(df_with_schema_v1.schema)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f304b93-216c-44aa-b181-07176c543550"},{"cell_type":"code","source":["# Handling Nested JSON Objects\n","import json\n","from pyspark.sql.functions import explode, col\n","\n","json_string_nested = \"\"\"\n","[\n","  {\"id\": 1, \"name\": \"Alice\", \"address\": {\"city\": \"Lahore\", \"zip\": \"54000\"}},\n","  {\"id\": 2, \"name\": \"Bob\", \"address\": {\"city\": \"Karachi\", \"zip\": \"74200\"}},\n","  {\"id\": 3, \"name\": \"Charlie\", \"address\": {\"city\": \"Islamabad\"}}\n","]\n","\"\"\"\n","\n","data_nested = json.loads(json_string_nested)\n","df_nested = spark.createDataFrame(data_nested)\n","df_nested.show(truncate=False)\n","# print(df_nested.schema)\n","\n","\n","df_nested_select = df_nested.select(\n","    col(\"id\"),\n","    col(\"name\"),\n","    col(\"address.city\").alias(\"address_city\"),\n","    col(\"address.zip\").alias(\"address_zip\")\n",")\n","\n","df_nested_select.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"100ead97-d505-486f-ae33-2d2f02b4a33a","normalized_state":"finished","queued_time":"2025-10-14T18:12:22.7039133Z","session_start_time":null,"execution_start_time":"2025-10-14T18:12:22.7050023Z","execution_finish_time":"2025-10-14T18:12:24.2583845Z","parent_msg_id":"cd3a36c8-3f06-4718-93e4-10b6f4794a39"},"text/plain":"StatementMeta(, 100ead97-d505-486f-ae33-2d2f02b4a33a, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------------------+---+-------+\n|             address| id|   name|\n+--------------------+---+-------+\n|{zip -> 54000, ci...|  1|  Alice|\n|{zip -> 74200, ci...|  2|    Bob|\n| {city -> Islamabad}|  3|Charlie|\n+--------------------+---+-------+\n\n+---+-------+------------+-----------+\n| id|   name|address_city|address_zip|\n+---+-------+------------+-----------+\n|  1|  Alice|      Lahore|      54000|\n|  2|    Bob|     Karachi|      74200|\n|  3|Charlie|   Islamabad|       NULL|\n+---+-------+------------+-----------+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11252cdc-3bcb-4f1e-98a6-8e5d34bdd0b7"},{"cell_type":"code","source":["# Handling JSON Arrays\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, explode_outer\n","import json\n","\n","json_string_array = \"\"\"\n","[\n","  {\"id\": 1, \"name\": \"Alice\", \"phones\": [\"0300-1111111\", \"0300-2222222\"]},\n","  {\"id\": 2, \"name\": \"Bob\",   \"phones\": [\"0300-3333333\"]},\n","  {\"id\": 3, \"name\": \"Charlie\", \"phones\": []},\n","  {\"id\": 4, \"name\": \"Diana\"}\n","]\n","\"\"\"\n","\n","data_array = json.loads(json_string_array)\n","df_array = spark.createDataFrame(data_array)\n","\n","# df_array.printSchema()\n","# df_array.show(truncate=False)\n","\n","df_array.select(\n","    col(\"id\"),\n","    col(\"name\"),\n","    col(\"phones\")[0].alias(\"primary_phone\"),\n","    col(\"phones\")[1].alias(\"secondary_phone\")\n",").show(truncate=False)\n","\n","\n","df_exploded = df_array.withColumn(\"phone\", explode(col(\"phones\"))) \\\n","                      .select(col(\"id\"), col(\"name\"), col(\"phone\"))\n","df_exploded.show()\n","\n","df_exploded = df_array.withColumn(\"phone\", explode_outer(col(\"phones\"))) \\\n","                      .select(\"id\", \"name\", \"phone\")\n","\n","# df_exploded.show()\n","\n","# explode() vs explode_outer():\n","# explode()        â†’ drops rows with empty or null arrays\n","# explode_outer()  â†’ keeps them, fills phone=null\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"5ca48b54-9b50-432f-b776-054b9e08cd04","normalized_state":"finished","queued_time":"2025-10-10T18:51:28.1100422Z","session_start_time":null,"execution_start_time":"2025-10-10T18:51:28.1112359Z","execution_finish_time":"2025-10-10T18:51:29.6267454Z","parent_msg_id":"66ef64b5-7d01-4e61-9a38-89fb71a3ffda"},"text/plain":"StatementMeta(, 5ca48b54-9b50-432f-b776-054b9e08cd04, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+-------+-------------+---------------+\n|id |name   |primary_phone|secondary_phone|\n+---+-------+-------------+---------------+\n|1  |Alice  |0300-1111111 |0300-2222222   |\n|2  |Bob    |0300-3333333 |NULL           |\n|3  |Charlie|NULL         |NULL           |\n|4  |Diana  |NULL         |NULL           |\n+---+-------+-------------+---------------+\n\n+---+-----+------------+\n| id| name|       phone|\n+---+-----+------------+\n|  1|Alice|0300-1111111|\n|  1|Alice|0300-2222222|\n|  2|  Bob|0300-3333333|\n+---+-----+------------+\n\n"]}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"274627f2-2c48-46de-a05c-2d9f7576aa0f"},{"cell_type":"code","source":["#  Reading JSON from Files\n","df_single_line = spark.read.json(\"Files/client_input_data/json/single_line_students.json\")\n","df_single_line.show()\n","\n","df_multi_line = spark.read.option(\"multiline\", \"true\").json(\"Files/client_input_data/json/multi_line_students.json\")\n","df_multi_line.show()\n","\n","output_path_single = \"Files/client_output_data/json/output_single_line\"\n","output_path_multi  = \"Files/client_output_data/json/output_multi_line\"\n","\n","df_single_line.write.mode(\"overwrite\").json(output_path_single)\n","\n","df_multi_line.coalesce(1) \\\n","    .write.mode(\"overwrite\") \\\n","    .option(\"multiline\", \"true\") \\\n","    .json(output_path_multi)\n","\n","print(df_multi_line.schema.json())  # Save for later use\n","# df_fixed = spark.read.schema(schema).json(\"Files/...\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"b2276c1b-c42b-4b2c-a631-836fcb7210c3","normalized_state":"finished","queued_time":"2025-10-10T10:04:58.9205505Z","session_start_time":null,"execution_start_time":"2025-10-10T10:04:58.9216385Z","execution_finish_time":"2025-10-10T10:05:03.9365862Z","parent_msg_id":"215aee3c-325f-4b07-82c1-2c7d035509d1"},"text/plain":"StatementMeta(, b2276c1b-c42b-4b2c-a631-836fcb7210c3, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+---+-------+\n|age| id|   name|\n+---+---+-------+\n| 25|  1|  Alice|\n| 30|  2|    Bob|\n| 35|  3|Charlie|\n+---+---+-------+\n\n+---+---+--------+\n|age| id|    name|\n+---+---+--------+\n| 35|  1|  Shamas|\n| 40|  2|   Imran|\n| 45|  3|Muhammad|\n| 50|  4|   Irfan|\n+---+---+--------+\n\n{\"fields\":[{\"metadata\":{},\"name\":\"age\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fab0b510-4023-4a2e-8dfc-326073269695"},{"cell_type":"markdown","source":["## ðŸ§© Single-line JSON vs Multi-line JSON\n","---\n","Each record (JSON object) is on **its own line**.  \n","This format is common in **logs**, **streaming data**, and **data lakes**."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25e34f20-567b-4de3-b40d-76fbbcb91bde"},{"cell_type":"code","source":["# Schema Drift Over Time (Evolution)\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lit\n","\n","spark = SparkSession.builder.appName(\"SchemaDriftExample\").getOrCreate()\n","\n","# Create first DataFrame (original schema)\n","data_v1 = [(1, \"Ali\", \"Pakistan\"), (2, \"Sara\", \"USA\")]\n","df_v1 = spark.createDataFrame(data_v1, [\"id\", \"name\", \"country\"])\n","\n","# Write version 1 to Parquet\n","output_path = \"Files/client_output_data/parquet/schema_drift\"\n","df_v1.write.mode(\"overwrite\").parquet(output_path)\n","\n","# Create second DataFrame (schema evolved: new column added)\n","data_v2 = [(3, \"Ahmed\", \"UK\", 2025), (4, \"Maria\", \"Canada\", 2025)]\n","df_v2 = spark.createDataFrame(data_v2, [\"id\", \"name\", \"country\", \"year\"])\n","\n","# Append version 2 to the same Parquet folder (enable schema merge)\n","df_v2.write.mode(\"append\").option(\"mergeSchema\", \"true\").parquet(output_path)\n","\n","# Read combined data with merged schema\n","df_combined = spark.read.option(\"mergeSchema\", \"true\").parquet(output_path)\n","\n","df_combined.show()\n","df_combined.printSchema()\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d2b73b82-8c7e-4ebf-9170-197358fd7b06"},{"cell_type":"code","source":["import json\n","from pyspark.sql.functions import explode, col\n","\n","# 1) JSON variable (array of objects, multi-line)\n","json_variable = \"\"\"\n","{\n","  \"metadata\": {\n","    \"version\": \"1.0\",\n","    \"source\": \"user_generated\",\n","    \"created_at\": \"2025-10-08T18:55:00Z\"\n","  },\n","  \"users\": [\n","    {\n","      \"id\": 1,\n","      \"name\": \"Alice\",\n","      \"age\": 30,\n","      \"address\": {\"street\": \"Main St\", \"city\": \"NY\"},\n","      \"phones\": [\"123-4567\", \"987-6543\"]\n","    },\n","    {\n","      \"id\": 2,\n","      \"name\": \"Bob\",\n","      \"address\": {\"street\": \"2nd St\", \"city\": \"LA\"},\n","      \"phones\": [\"555-5555\"],\n","      \"email\": \"bob@example.com\"\n","    }\n","  ]\n","}\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"100ead97-d505-486f-ae33-2d2f02b4a33a","normalized_state":"finished","queued_time":"2025-10-14T18:22:48.9322242Z","session_start_time":null,"execution_start_time":"2025-10-14T18:22:48.9334459Z","execution_finish_time":"2025-10-14T18:22:49.2577795Z","parent_msg_id":"bd61243e-bf84-413e-8b18-5ae23fdeca90"},"text/plain":"StatementMeta(, 100ead97-d505-486f-ae33-2d2f02b4a33a, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03e31c7c-66d4-4d86-abab-ed3f2cd16fa6"},{"cell_type":"code","source":["# Parse JSON\n","data = json.loads(json_variable)\n","\n","print(\"=== Users Data ===\") # array\n","print(data[\"users\"])\n","\n","print(\"=== Metadata ===\") # dictionary\n","print(data[\"metadata\"])\n","\n","users_with_metadata = []\n","\n","for user in data[\"users\"]:\n","    print(\"Original user:\", user)\n","    # make a copy so the original dictionary isn't changed\n","    new_user = user.copy()\n","    print(\"   Copied user:\", new_user)\n","    # add metadata info to the user\n","    new_user.update(data[\"metadata\"]) # merge metadata to new_user\n","    print(\"   After adding metadata:\", new_user)\n","    # add the updated user to the final list\n","    users_with_metadata.append(new_user)\n","    print(\"Added to final list.\\n\")\n","\n","print(\"Final users_with_metadata list:\")\n","print(users_with_metadata)\n","\n","# Flatten: merge metadata into each user\n","# users_with_metadata = [ {**user, **data[\"metadata\"]} for user in data[\"users\"] ]\n","\n","# Create PySpark DataFrame\n","df = spark.createDataFrame(users_with_metadata)\n","\n","print(\"=== Flattened DataFrame ===\")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"100ead97-d505-486f-ae33-2d2f02b4a33a","normalized_state":"finished","queued_time":"2025-10-14T18:26:03.8643852Z","session_start_time":null,"execution_start_time":"2025-10-14T18:26:03.8654561Z","execution_finish_time":"2025-10-14T18:26:22.8714128Z","parent_msg_id":"e29e0651-ccab-46b6-9c2f-759ace716820"},"text/plain":"StatementMeta(, 100ead97-d505-486f-ae33-2d2f02b4a33a, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== Users Data ===\n[{'id': 1, 'name': 'Alice', 'age': 30, 'address': {'street': 'Main St', 'city': 'NY'}, 'phones': ['123-4567', '987-6543']}, {'id': 2, 'name': 'Bob', 'address': {'street': '2nd St', 'city': 'LA'}, 'phones': ['555-5555'], 'email': 'bob@example.com'}]\n=== Metadata ===\n{'version': '1.0', 'source': 'user_generated', 'created_at': '2025-10-08T18:55:00Z'}\nOriginal user: {'id': 1, 'name': 'Alice', 'age': 30, 'address': {'street': 'Main St', 'city': 'NY'}, 'phones': ['123-4567', '987-6543']}\n   Copied user: {'id': 1, 'name': 'Alice', 'age': 30, 'address': {'street': 'Main St', 'city': 'NY'}, 'phones': ['123-4567', '987-6543']}\n   After adding metadata: {'id': 1, 'name': 'Alice', 'age': 30, 'address': {'street': 'Main St', 'city': 'NY'}, 'phones': ['123-4567', '987-6543'], 'version': '1.0', 'source': 'user_generated', 'created_at': '2025-10-08T18:55:00Z'}\nAdded to final list.\n\nOriginal user: {'id': 2, 'name': 'Bob', 'address': {'street': '2nd St', 'city': 'LA'}, 'phones': ['555-5555'], 'email': 'bob@example.com'}\n   Copied user: {'id': 2, 'name': 'Bob', 'address': {'street': '2nd St', 'city': 'LA'}, 'phones': ['555-5555'], 'email': 'bob@example.com'}\n   After adding metadata: {'id': 2, 'name': 'Bob', 'address': {'street': '2nd St', 'city': 'LA'}, 'phones': ['555-5555'], 'email': 'bob@example.com', 'version': '1.0', 'source': 'user_generated', 'created_at': '2025-10-08T18:55:00Z'}\nAdded to final list.\n\nFinal users_with_metadata list:\n[{'id': 1, 'name': 'Alice', 'age': 30, 'address': {'street': 'Main St', 'city': 'NY'}, 'phones': ['123-4567', '987-6543'], 'version': '1.0', 'source': 'user_generated', 'created_at': '2025-10-08T18:55:00Z'}, {'id': 2, 'name': 'Bob', 'address': {'street': '2nd St', 'city': 'LA'}, 'phones': ['555-5555'], 'email': 'bob@example.com', 'version': '1.0', 'source': 'user_generated', 'created_at': '2025-10-08T18:55:00Z'}]\n=== Flattened DataFrame ===\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"a67daf31-ad4d-4bb5-b8f8-878d8be8df5d","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, a67daf31-ad4d-4bb5-b8f8-878d8be8df5d)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f3db781b-cd2a-43d3-accb-68213bdb09d6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"a67daf31-ad4d-4bb5-b8f8-878d8be8df5d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":{"city":"NY","street":"Main St"},"1":"30","2":"2025-10-08T18:55:00Z","3":"1","4":"Alice","5":["123-4567","987-6543"],"6":"user_generated","7":"1.0"},{"0":{"city":"LA","street":"2nd St"},"2":"2025-10-08T18:55:00Z","3":"2","4":"Bob","5":["555-5555"],"6":"user_generated","7":"1.0","8":"bob@example.com"}],"schema":[{"key":"0","name":"address","type":"MapType(StringType,StringType,true)"},{"key":"1","name":"age","type":"bigint"},{"key":"2","name":"created_at","type":"string"},{"key":"3","name":"id","type":"bigint"},{"key":"4","name":"name","type":"string"},{"key":"5","name":"phones","type":"ArrayType(StringType,true)"},{"key":"6","name":"source","type":"string"},{"key":"7","name":"version","type":"string"},{"key":"8","name":"email","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"10e93dd2-c53f-4016-a40b-fc1291bfc259"}],"default_lakehouse":"10e93dd2-c53f-4016-a40b-fc1291bfc259","default_lakehouse_name":"test_Lakehouse","default_lakehouse_workspace_id":"f82b1a38-4189-4ce2-9a6b-150c49e1a419"}}},"nbformat":4,"nbformat_minor":5}